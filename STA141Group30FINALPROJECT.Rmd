---
title: "Data Description - Bank Marketing"
author: "Zheng Sun, Xingyang Lan, Shan Xu"
date: "10/21/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(datasets)
library(ISLR)
library(glmnet)
library(dplyr)
library(tidyr)
library(ISLR)           
library(rpart)          
library(rpart.plot)
library(randomForest)   
library(gbm) 
library(MASS)
library(caret)
library(pROC)
library(PresenceAbsence)
library(e1071)
```
# Literature Review

  Sergio Mora, Paulo Cortez, and Paul Rita predict the bank customers who would subscribe to a bank deposit. They compared four four different Data mining models including Logistic Regression, Decision Trees, Neural Network, and Support Vector Machine (June 2014).  We have decided to use Lasso Regression, Random Forest, Logistic Regression models to fit the training data throughout the personal, contact, and socioeconomic variables of the dataset, and test their accuracy. 

  The first model that we will be using is Lasso Regression since the dataset contains many input variables, and LASSO regresion can result in a sparse model and enhance the prediction accuracy and interpretability of the statistical model.

  The next model we will be using is Random Forest. A general strength for decision trees in this particular data set is that it is believed that decision trees closely mirror human decision making. Random Forest is able to decorrelate trees; but we note that random forest does underperform other models such as Lasso when attempting to draw a decision boundary drawn on a likelyhood that is continuously improved by many discrete variables; such as a quadratic decision boundary.

  Logistic regression is a predictive modeling algorithm that is used when the Y variable is binary categorical.it can take only two values like 1 or 0. so, we can use logistic regression for our data.
# Data Exploration
## need some adjust here
```{r, echo=FALSE}
bank <- read.table("bank-additional-full.csv", head = T, sep = ";")
head(bank)
par(mfrow=c(2,2))
boxplot(bank$age,main="Age")
pie(table(bank$job),main="Job")
pie(table(bank$marital),main="Marital")
pie(table(bank$education),main="Education")
par(mfrow=c(2,2))
pie(table(bank$default),main="Credit in Default")
pie(table(bank$housing),main="Housing Loan")
pie(table(bank$loan),main="Personal Loan")
pie(table(bank$contact),main="Contact")
par(mfrow=c(2,2))
pie(table(bank$poutcome),main="outcome of the previous marketing campaign")
pie(table(bank$y),main="Subscribe")
boxplot(bank$campaign,main="Number of Contacts Performed During this Campaign")
boxplot(bank$emp.var.rate,main="Employment Variation Rate")
par(mfrow=c(2,2))
boxplot(bank$cons.price.idx,main="Consumer Price Index ")
boxplot(bank$cons.conf.idx,main="Consumer Confidence Index")
boxplot(bank$euribor3m,main="Euribor 3 Month Rate")
boxplot(bank$nr.employed,main="Number of Employees")
```
# Model Selection

We will test various data mining algorithm to see how these input variables can predit our y output to see if the consumers will subscribe long term loan. We will compare each model and select the best one. 

# LASSO Regression
We begin with the Lasso Regression, using 30% data as training data, 70% as test.
The dataset contains many input variables. LASSO Regreesion can use shrinkage on some variables and produce sparse model.

```{r}
rm(list = ls())
bank = read.table("bank-additional-full.csv", head = T, sep = ";")
bank = bank %>%
  mutate(y = ifelse(y == "no",0,1))

bank = subset(bank, select=c(age,campaign,pdays,duration,previous,emp.var.rate, cons.price.idx, cons.conf.idx,euribor3m, nr.employed,y))
sample = sample.int(n = nrow(bank), size = floor(0.3*nrow(bank)), replace = F)
train = bank[sample, ]
test = bank[-sample, ]
x_train = model.matrix(y~., train)[,-1]
x_test = model.matrix(y~., test)[,-1]
y_train = train$y
y_test = test$y
```
We only consider numerical data here, and first use 30% data to train and the 70% data to test.
```{r}
lasso_mod = glmnet(x_train ,y_train, alpha = 1)
cv.out = cv.glmnet(x_train, y_train, alpha = 1, nfold=10)
plot(cv.out) 
bestlam = cv.out$lambda.min 
lasso_best = glmnet(x_train ,y_train, alpha = 1, lambda = bestlam)
a = predict(lasso_best, x_train) 
myroc = roc(y_train, as.numeric(a))
mycoords = coords(myroc, x = "best",transpose = FALSE)
```
We use Cross Validation to select the best lambda. As a result, the LASSO model is $avg[L(outcome, \beta_0 + \beta_1 x_1 + ... + \beta_k x_k)] + \lambda P (\beta_1, . . . , \beta_k)$, where $\lambda$ = `r bestlam`, and  $\beta$ is
```{r}
coef(lasso_best)
```
By maximizing (specificity + sensitivity), we can determine the threshold. Then if the output produced by lasso model is larger then the `r mycoords[1,1]`, it will be considered as 1(yes), otherwise it is 0(No).
```{r}
mycoords
```
Then we can use the trainning lasso model and the threshold to test the testing data.
```{r}
lasso_pred = predict(lasso_best, newx = x_test) 
confusion_matrix = table(y_test, as.numeric(lasso_pred>mycoords[1,1]))
accuracy = (confusion_matrix[1,1] + confusion_matrix[2,2])/length(y_test)
```
The result is:
```{r}
accuracy
confusion_matrix
```
The roc for this model is:
```{r}
lasso_pred = predict(lasso_mod, s = bestlam, newx = x_test) 
roc_lasso_test=roc(response = y_test, predictor =as.numeric(lasso_pred))
plot(roc_lasso_test, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="skyblue", print.thres=TRUE,main='LASSO ROC')
```
## Random Forest

Next we create the Random Forest model, with 40% data for training and 60% for testing.
```{r}
rm(list=ls())
library(randomForest)
require(randomForest)
set.seed(1)

bank.additional.full <- read.csv("~/Downloads/bank-additional-full.csv", header=TRUE, sep=";")
head(bank.additional.full)
bank <- bank.additional.full[2:41189,]
colnames(bank) <- c("age", "job", "marital", "education", "default","housing","loan","contact","month","day_of_week","duration","campaign","pdays","previous","poutcome","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed","y")

```
For the application of Random Forest on this dataset, after a comparison to changing unknowns to NA and replacing unknowns with the average value, the model performs better on the ROC curve predictively when the unknowns are simply removed.
```{r}

unknowns = which(apply(bank, 1, function(r) any(r %in% c("unknown"))))
bank <- bank[-unknowns,]
```
This random forest model uses all of the dependent variables, both categorical and numerical. Restricting the model to only the numerical and binomial converted to numerical variables results in an approximately 20% decrease in the true positive rate.
```{r}
bank[,"y"]=ifelse(bank[,"y"]=="yes",1,0)
bank$y <- as.integer(bank$y)
bank$y <- as.factor(bank$y)
bank <- droplevels(bank)
```
Now we section out the 40% training data.
```{r}
train <- sample(2:nrow(bank),0.4*nrow(bank))
```
We set up the Random Forest model after organization of the data and factor levels.
```{r}
rf.bank = randomForest(formula = y~., data = bank, subset = train, mtry=6, importance=T)
rf.bank
importance(rf.bank)
#summary(rf.bank)
varImpPlot(rf.bank)
plot(rf.bank)
```
New we use the data set aside for testing and use it the model to make predictions.
```{r}
pred1 = predict(rf.bank, bank[-train,], type = "response", ordered = FALSE)
pred1 <-as.numeric(pred1)
confusionM = table(pred1, bank$y[-train])
accuracy = (confusionM[1,1]+confusionM[2,2])/(sum(confusionM))

```
I looped over mtry = 1:18 and had the best out of bag error rate at mtry = 6,
The model at mtry = 6 has an out of bag error rate of:
```{r}
oob.err = 0
oob.err <- rf.bank$err.rate[nrow(rf.bank$err.rate),1]
oob.err
```
The accuracy is:
```{r}
accuracy
```
The confusion matrix is:
```{r}
confusionM
```
and the ROC for the Random Forest model is:
```{r}
the.roc = roc(response = bank$y[-train], predictor = pred1)
plot.roc(the.roc, axes = TRUE, legacy.axes = FALSE, print.auc=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE,auc.polygon.col="skyblue", print.thres=TRUE,main='RF ROC')
```
## Logistic Regression

I set 50% of data as training data, else data for test.
```{r}
bank = subset(bank, select=c(age,campaign,pdays,duration,
                             previous,emp.var.rate, cons.price.idx, 
                             cons.conf.idx,euribor3m, nr.employed,y))
sample <- sample.int(n = nrow(bank), size = floor(0.5*nrow(bank)), replace = F)
train = bank[sample, ]
test = bank[-sample, ]
x_train = model.matrix(y~., train)[,-1]
x_test = model.matrix(y~., test)[,-1]
y_train = train$y
y_test = test$y

```

logistic regression achieves by taking the log odds of the even $ln(P_i/1-P_i)$, where p is the probability of event (always between 0 and 1). the logisitis equation is 
$Z_i=ln(p_i/1-p_i)=\alpha+\beta_1x_1+ ...+\beta_nx_n$
Taking exponent on both sides of equation gives:
$P_i=E(y=1|x_i)= e^z/1+e^z=e^{\alpha+\beta_i x_i}/{1-e^{\alpha+\beta_i x_i}}$
this equation works for glm() function. 

let's fit the model. For the logistic regression, use glm() function and use parameter family=bionmail in the model. And, using function summary() to obtain the results of the model.
```{r}
log_mod=glm(y~age+campaign+pdays+duration+previous+emp.var.rate+ cons.price.idx+
              cons.conf.idx+euribor3m+ nr.employed,data=train, family = "binomial")
```
Then, predict the test data based on the train model. I set the probability threshold value as 0.5. The confusion Matrix is:
```{r}
loc_pred=predict(log_mod,data=test,type="response")
loc_pred=ifelse(loc_pred>0.5,1,0)
table(predict(log_mod,test)>0.5,test$y)

loc_accuracy=mean(loc_pred==test$y)
```
And the accuracy is:
```{r}
loc_accuracy
```
The plot for the ROC is:
```{r}
test_prob=predict(log_mod,test,type="response")
roc(test$y~test_prob,plot=TRUE,print.auc=TRUE)
```
## Additional Method: SVM

  We also can use svm here. The output y in the dataset is either 0 or 1, so we can set a hyperplane that divides the data into two part, where one part is yes and another is no.

```{r}
bank = subset(bank, select=c(age,campaign,pdays,duration,previous,emp.var.rate, cons.price.idx, cons.conf.idx,euribor3m, nr.employed,y))
sample <- sample.int(n = nrow(bank), size = floor(0.3*nrow(bank)), replace = F)
train = bank[sample, ]
test = bank[-sample, ]
x_train = model.matrix(y~., train)[,-1]
x_test = model.matrix(y~., test)[,-1]
y_train = factor(train$y)
y_test = test$y
svm_model = svm(factor(y) ~. , data=train, kernel = "radial")
w = t(svm_model$coefs) %*% svm_model$SV
b = -svm_model$rho
```
We first built the SVM and use radial basis kernel to train.

Then we can use the trained svm to test.
```{r}
svm_pred =  predict(svm_model,x_test)
confusion_matrix = table(y_test, svm_pred)
accuracy = (confusion_matrix[1,1] + confusion_matrix[2,2])/length(y_test)
```
The result is:
```{r}
accuracy
confusion_matrix
```
The roc for the SVM model is
```{r}
roc_svm_test=roc(response = y_test, predictor =as.numeric(svm_pred))
plot(roc_svm_test, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="skyblue", print.thres=TRUE,main='SVM ROC')
```
# Summary and Model Comparison

  Based on the plot in the Data Exploration, we noticed that the most common response for consumer subscription to long term loans is “no”. Due to the large proportion responding "no", the accuracy can be relatively high but not indicative of a good true positive rate. We emphasized the true positive rate for long term subscribing consumers in our model performance valuations; the true positives being consumers that actually subscribed to long term loans when our individual models predicted that they would.

  Acknowledging the inherent tradeoff in models for sensitivity versus specificity, we used the AUC regions to decide which models performed better on the dataset.

  The SVM model performed poorly here, since it simply uses one hyperplane to divide the data into two parts. However, the dataset is not dimidiated.

  The Random forest performed better than the SVM model and results in the highest accuracy, but its AUC was not nearly as high as that of the LASSO or logistic regression models. This is due to that the model prioritized accuracy, and we cannot set a threshold to increase its true positive rate since it will decrease the accuracy. However, we were able to impliment a mild increase in both accuracy and true positive rate through optimizing over the random selection of m predictors, mtry, with a loop. 

  The LASSO and Logistic Regression models performed similarily. After creating their training models, we found and implemented the best coefficients for the threshold that maximizes the sum of specificity and sensitivity of the training data. Then we can use this threshold in our predictions to test the model on the test data, which will result in a higher true positive rate.

  Finally, we conclude that LASSO Regression is the best model for this dataset as it resulted in a sparse model; performing well with a distribution of 40% training and 60% testing data. The accuracy of LASSO Regression is 83.05%, , the specificity (TNR) is 82.46% and the sensitivity (TPR) is 87.72%.


